{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianellisjones/usn/blob/main/Fleet_Tracker_Map_(Improved).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "US NAVY UNIFIED FLEET INTEL (Official + OSINT)\n",
        "\n",
        "A comprehensive intelligence collector that runs two parallel operations:\n",
        "1. OFFICIAL: Scrapes uscarriers.net for verified deployment history (CVN/LHA/LHD).\n",
        "2. OSINT: Scrapes WarshipCam and Search Engines for real-time social sightings.\n",
        "\n",
        "Outputs two CSV files: 'big_deck_status.csv' and 'social_naval_intel.csv'.\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "\n",
        "# --- OFFICIAL SOURCES (USCarriers.net) ---\n",
        "FLEET_URLS: List[str] = [\n",
        "    # Aircraft Carriers (CVN)\n",
        "    \"http://uscarriers.net/cvn68history.htm\", # USS Nimitz\n",
        "    \"http://uscarriers.net/cvn69history.htm\", # USS Eisenhower\n",
        "    \"http://uscarriers.net/cvn70history.htm\", # USS Carl Vinson\n",
        "    \"http://uscarriers.net/cvn71history.htm\", # USS Theodore Roosevelt\n",
        "    \"http://uscarriers.net/cvn72history.htm\", # USS Abraham Lincoln\n",
        "    \"http://uscarriers.net/cvn73history.htm\", # USS George Washington\n",
        "    \"http://uscarriers.net/cvn74history.htm\", # USS John C. Stennis\n",
        "    \"http://uscarriers.net/cvn75history.htm\", # USS Harry S. Truman\n",
        "    \"http://uscarriers.net/cvn76history.htm\", # USS Ronald Reagan\n",
        "    \"http://uscarriers.net/cvn77history.htm\", # USS George H.W. Bush\n",
        "    \"http://uscarriers.net/cvn78history.htm\", # USS Gerald R. Ford\n",
        "\n",
        "    # Amphibious Assault Ships (LHA/LHD)\n",
        "    \"http://uscarriers.net/lhd1history.htm\", # USS Wasp\n",
        "    \"http://uscarriers.net/lhd2history.htm\", # USS Essex\n",
        "    \"http://uscarriers.net/lhd3history.htm\", # USS Kearsarge\n",
        "    \"http://uscarriers.net/lhd4history.htm\", # USS Boxer\n",
        "    \"http://uscarriers.net/lhd5history.htm\", # USS Bataan\n",
        "    \"http://uscarriers.net/lhd7history.htm\", # USS Iwo Jima\n",
        "    \"http://uscarriers.net/lhd8history.htm\", # USS Makin Island\n",
        "    \"http://uscarriers.net/lha6history.htm\", # USS America\n",
        "    \"http://uscarriers.net/lha7history.htm\", # USS Tripoli\n",
        "]\n",
        "\n",
        "# --- OSINT SOURCES ---\n",
        "WARSHIPCAM_URL = \"https://www.warshipcam.net/\"\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 1: OFFICIAL TRACKER LOGIC\n",
        "# ==============================================================================\n",
        "\n",
        "def fetch_history_text(url: str, char_limit: int = 50000) -> str:\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': USER_AGENT}, timeout=20)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        full_text = soup.get_text(separator='\\n')\n",
        "        lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
        "        clean_text = '\\n'.join(lines)\n",
        "        return clean_text[-char_limit:] if len(clean_text) > char_limit else clean_text\n",
        "    except Exception as e:\n",
        "        return f\"ERROR: {str(e)}\"\n",
        "\n",
        "def parse_status_entry(text_block: str) -> Tuple[str, str]:\n",
        "    lines = text_block.split('\\n')\n",
        "    current_year = \"Unknown\"\n",
        "    years_found = re.findall(r'(202[3-6])', text_block)\n",
        "    if years_found:\n",
        "        priority_years = [y for y in years_found if y in ['2024', '2025']]\n",
        "        current_year = priority_years[-1] if priority_years else years_found[-1]\n",
        "\n",
        "    processed_lines = []\n",
        "    running_year = current_year\n",
        "    for line in lines:\n",
        "        year_match = re.search(r'^202[3-6]', line)\n",
        "        if year_match: running_year = year_match.group(0)\n",
        "        processed_lines.append({'text': line, 'year': running_year})\n",
        "\n",
        "    keywords = [\"moored\", \"anchored\", \"underway\", \"arrived\", \"departed\", \"transited\", \"operations\", \"returned\", \"participated\", \"conducted\", \"moved to\", \"visited\", \"pulled into\", \"sea trials\", \"flight deck certification\"]\n",
        "    allowed_years = [\"2024\", \"2025\", \"2026\"]\n",
        "\n",
        "    for entry in reversed(processed_lines):\n",
        "        text_lower = entry['text'].lower()\n",
        "        year = entry['year']\n",
        "        if year in allowed_years and any(k in text_lower for k in keywords):\n",
        "            if text_lower.strip().startswith(\"from \") and \" - \" in text_lower: continue\n",
        "            return year, entry['text']\n",
        "    return current_year, \"No status found.\"\n",
        "\n",
        "def categorize_location(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    if \"departed san diego\" in text: return \"Pacific Ocean\"\n",
        "    if \"departed norfolk\" in text: return \"Atlantic Ocean\"\n",
        "    if \"departed pearl harbor\" in text: return \"Pacific Ocean\"\n",
        "    if \"departed mayport\" in text: return \"Atlantic Ocean\"\n",
        "\n",
        "    location_map = {\n",
        "        \"Norfolk / Portsmouth\": [\"norfolk\", \"portsmouth\", \"virginia beach\", \"nassco\"],\n",
        "        \"San Diego\": [\"san diego\", \"north island\", \"camp pendleton\"],\n",
        "        \"Bremerton / Kitsap\": [\"bremerton\", \"kitsap\"],\n",
        "        \"Newport News\": [\"newport news\"],\n",
        "        \"Yokosuka\": [\"yokosuka\"],\n",
        "        \"Pearl Harbor\": [\"pearl harbor\"],\n",
        "        \"Mayport\": [\"mayport\"],\n",
        "        \"Everett\": [\"everett\"],\n",
        "        \"Singapore\": [\"singapore\", \"changi\"],\n",
        "        \"Bahrain\": [\"bahrain\", \"manama\"],\n",
        "        \"Dubai\": [\"dubai\", \"jebel ali\"],\n",
        "        \"Busan\": [\"busan\"],\n",
        "        \"Guam\": [\"guam\", \"apra\"],\n",
        "        \"Sasebo\": [\"sasebo\", \"juliet basin\"],\n",
        "        \"Malaysia\": [\"malaysia\", \"klang\"],\n",
        "        \"Philippines\": [\"philippines\", \"manila\", \"subic\"],\n",
        "        \"Pascagoula\": [\"pascagoula\"],\n",
        "        \"South China Sea\": [\"south china sea\", \"spratly islands\", \"luzon\"],\n",
        "        \"Western Pacific (WESTPAC)\": [\"san bernardino strait\", \"western pacific\", \"westpac\"],\n",
        "        \"Red Sea\": [\"red sea\"],\n",
        "        \"Persian Gulf\": [\"persian gulf\", \"arabian gulf\"],\n",
        "        \"Gulf of Oman\": [\"gulf of oman\"],\n",
        "        \"Gulf of Aden\": [\"gulf of aden\"],\n",
        "        \"Mediterranean\": [\"mediterranean\"],\n",
        "        \"Caribbean Sea\": [\"caribbean\", \"st. croix\", \"trinidad\", \"tobago\", \"puerto rico\"],\n",
        "        \"North Sea\": [\"north sea\"],\n",
        "        \"Norwegian Sea\": [\"norwegian sea\"],\n",
        "        \"Strait of Gibraltar\": [\"gibraltar\"],\n",
        "        \"Suez Canal\": [\"suez\"],\n",
        "        \"Bab el-Mandeb\": [\"bab el-mandeb\"],\n",
        "        \"Philippine Sea\": [\"philippine sea\", \"okinawa\"],\n",
        "        \"Atlantic Ocean\": [\"atlantic\"],\n",
        "        \"Pacific Ocean\": [\"pacific\"],\n",
        "        \"Indian Ocean\": [\"indian ocean\"],\n",
        "    }\n",
        "    for label, keywords in location_map.items():\n",
        "        if any(k in text for k in keywords): return label\n",
        "    return \"Underway / Unknown\"\n",
        "\n",
        "def extract_date(text: str) -> str:\n",
        "    pattern = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2}'\n",
        "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "    return matches[-1] if matches else \"Date Unspecified\"\n",
        "\n",
        "# ==============================================================================\n",
        "# MODULE 2: SOCIAL INTEL LOGIC\n",
        "# ==============================================================================\n",
        "\n",
        "class IntelParser:\n",
        "    @staticmethod\n",
        "    def clean_text(text: str) -> str:\n",
        "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "        text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
        "        text = re.sub(r'(?i)posted|shared by|likes|comments|src:|twitter|instagram|facebook', '', text)\n",
        "        text = re.sub(r'^\\d{1,2}, 202', '202', text)\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_ship_details(text: str) -> Tuple[Optional[str], Optional[str], str]:\n",
        "        ship_pattern = r\"(USS\\s+[A-Z][a-z]+(?:\\s+[A-Z]\\.?\\s?[a-z]*){0,3}(?:\\s+\\([A-Za-z]+[-\\s]?\\d+\\))?)\"\n",
        "        match = re.search(ship_pattern, text)\n",
        "        if match:\n",
        "            full_ship_str = match.group(1)\n",
        "            hull_match = re.search(r'\\(([A-Za-z]+[-\\s]?\\d+)\\)', full_ship_str)\n",
        "            hull = hull_match.group(1).replace(\" \", \"\") if hull_match else \"Unknown\"\n",
        "            ship_name = re.sub(r'\\s+\\(.*\\)', '', full_ship_str).strip()\n",
        "            context = text.replace(full_ship_str, \"\").strip()\n",
        "            context = re.sub(r'^[-:,.]+\\s*', '', context)\n",
        "            return ship_name, hull, context\n",
        "        return None, None, text\n",
        "\n",
        "def scrape_warshipcam() -> List[Dict]:\n",
        "    print(f\"    > Scanning WarshipCam.net...\")\n",
        "    intel_report = []\n",
        "    try:\n",
        "        response = requests.get(WARSHIPCAM_URL, headers={'User-Agent': USER_AGENT}, timeout=20)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        raw_data = soup.get_text(\"\\n\", strip=True).split('\\n')\n",
        "        images = soup.find_all('img', alt=True)\n",
        "        for img in images:\n",
        "            if len(img['alt']) > 40: raw_data.append(img['alt'])\n",
        "\n",
        "        seen_texts = set()\n",
        "        date_pattern = re.compile(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2},?\\s+202[4-6]', re.IGNORECASE)\n",
        "\n",
        "        for text in raw_data:\n",
        "            if \"USS\" not in text: continue\n",
        "            cleaned = IntelParser.clean_text(text)\n",
        "            if cleaned in seen_texts: continue\n",
        "\n",
        "            date_match = date_pattern.search(text)\n",
        "            if date_match:\n",
        "                ship, hull, context = IntelParser.extract_ship_details(cleaned)\n",
        "                if ship:\n",
        "                    seen_texts.add(cleaned)\n",
        "                    intel_report.append({\n",
        "                        \"Source\": \"WarshipCam.net\",\n",
        "                        \"Date\": date_match.group(0),\n",
        "                        \"Ship\": ship,\n",
        "                        \"Hull\": hull,\n",
        "                        \"Activity/Location\": context[:150]\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"    ! Error scraping WarshipCam: {e}\")\n",
        "    return intel_report\n",
        "\n",
        "def duckduckgo_snipe(query: str) -> List[Dict]:\n",
        "    print(f\"    > Sniping DuckDuckGo for: '{query}'...\")\n",
        "    search_url = \"https://html.duckduckgo.com/html/\"\n",
        "    search_term = f\"WarshipCam {query}\"\n",
        "    intel_report = []\n",
        "    try:\n",
        "        time.sleep(random.uniform(1.5, 3.0))\n",
        "        response = requests.post(search_url, data={'q': search_term}, headers={'User-Agent': USER_AGENT}, timeout=15)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        results = soup.find_all('div', class_='result__body')\n",
        "        if not results: results = soup.find_all('a', class_='result__a')\n",
        "\n",
        "        for res in results:\n",
        "            text = res.get_text(\" \", strip=True)\n",
        "            if \"No results found\" in text: continue\n",
        "            if \"WarshipCam\" in text and \"USS\" in text:\n",
        "                cleaned = IntelParser.clean_text(text)\n",
        "                ship, hull, context = IntelParser.extract_ship_details(cleaned)\n",
        "                if ship:\n",
        "                    date_match = re.search(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2},?\\s+202[4-6]', text)\n",
        "                    date_str = date_match.group(0) if date_match else \"Recent (Search Snippet)\"\n",
        "                    intel_report.append({\n",
        "                        \"Source\": \"DuckDuckGo/Search\",\n",
        "                        \"Date\": date_str,\n",
        "                        \"Ship\": ship,\n",
        "                        \"Hull\": hull,\n",
        "                        \"Activity/Location\": context[:150]\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"    ! Error scraping DDG: {e}\")\n",
        "    return intel_report\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    print(f\"{'='*90}\")\n",
        "    print(f\"UNIFIED NAVAL INTELLIGENCE COLLECTOR (Official + OSINT)\")\n",
        "    print(f\"{'='*90}\\n\")\n",
        "\n",
        "    # --- PART 1: OFFICIAL SCRAPER ---\n",
        "    print(f\"[*] PHASE 1: OFFICIAL FLEET TRACKER (USCarriers.net)\")\n",
        "    official_data = []\n",
        "    for url in FLEET_URLS:\n",
        "        hull_match = re.search(r'((?:cvn|lha|lhd)\\d+)', url, re.IGNORECASE)\n",
        "        hull = hull_match.group(1).upper() if hull_match else \"UNK\"\n",
        "\n",
        "        raw_text = fetch_history_text(url)\n",
        "        if \"ERROR\" in raw_text:\n",
        "            year, status, loc_tag, date_str = \"Error\", raw_text, \"Error\", \"Error\"\n",
        "        else:\n",
        "            year, status = parse_status_entry(raw_text)\n",
        "            loc_tag = categorize_location(status)\n",
        "            date_str = extract_date(status)\n",
        "            if date_str == \"Date Unspecified\": date_str = year\n",
        "\n",
        "        official_data.append({\"Hull\": hull, \"Location\": loc_tag, \"Date\": date_str, \"Status Sentence\": status, \"Source URL\": url})\n",
        "        print(f\"    [{hull}] [{loc_tag}] [{date_str}] {status[:80]}...\")\n",
        "\n",
        "    # Write Official CSV\n",
        "    with open(\"big_deck_status.csv\", 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"Hull\", \"Location\", \"Date\", \"Status Sentence\", \"Source URL\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(official_data)\n",
        "    print(f\"    >>> Official data saved to 'big_deck_status.csv'\\n\")\n",
        "\n",
        "    # --- PART 2: SOCIAL INTEL ---\n",
        "    print(f\"[*] PHASE 2: SOCIAL INTELLIGENCE (OSINT)\")\n",
        "    web_intel = scrape_warshipcam()\n",
        "    search_intel = duckduckgo_snipe('\"USS\" \"2025\"')\n",
        "    all_intel = web_intel + search_intel\n",
        "\n",
        "    # Deduping Social Data\n",
        "    unique_ships = {}\n",
        "    for item in all_intel:\n",
        "        key = item['Ship']\n",
        "        if key not in unique_ships or len(item['Activity/Location']) > len(unique_ships[key]['Activity/Location']):\n",
        "            unique_ships[key] = item\n",
        "    social_data = list(unique_ships.values())\n",
        "\n",
        "    if social_data:\n",
        "        print(f\"    + Found {len(social_data)} unique social sightings.\")\n",
        "        with open(\"social_naval_intel.csv\", 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=[\"Source\", \"Date\", \"Ship\", \"Hull\", \"Activity/Location\"], extrasaction='ignore')\n",
        "            writer.writeheader()\n",
        "            writer.writerows(social_data)\n",
        "        print(f\"    >>> Social data saved to 'social_naval_intel.csv'\")\n",
        "    else:\n",
        "        print(\"    ! No social sightings found.\")\n",
        "\n",
        "    print(f\"\\n{'='*90}\")\n",
        "    print(f\"SUCCESS: Intelligence Cycle Complete.\")\n",
        "    print(f\"Ready to map with 'master_fleet_mapper.py'\")\n",
        "    print(f\"{'='*90}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "UNIFIED NAVAL INTELLIGENCE COLLECTOR (Official + OSINT)\n",
            "==========================================================================================\n",
            "\n",
            "[*] PHASE 1: OFFICIAL FLEET TRACKER (USCarriers.net)\n",
            "    [CVN68] [South China Sea] [Nov. 18] From November 8-11, the Nimitz CSG conducted operations  off the coast of Brunei...\n",
            "    [CVN69] [Norfolk / Portsmouth] [Jan. 8] September 26, USS Dwight D. Eisenhower moored at Pier 12N on Naval Station Norfo...\n",
            "    [CVN70] [San Diego] [September 16] September 16, The Carl Vinson moored at Juliet Pier on Naval Air Station North I...\n",
            "    [CVN71] [San Diego] [Nov. 10] November  8, USS Theodore Roosevelt moored at Berth Lima on Naval Air Station No...\n",
            "    [CVN72] [Pacific Ocean] [2025] , USS Abraham Lincoln departed San Diego for a scheduled deployment....\n",
            "    [CVN73] [South China Sea] [Nov. 22] From November 19-21, the George Washington CSG conducted operations northeast of...\n",
            "    [CVN74] [Newport News] [April 8] April 8, 2024 USS John C. Stennis undocked and moored at Outfitting Berth #1 on ...\n",
            "    [CVN75] [Norfolk / Portsmouth] [Oct. 8] September 26, The Harry S. Truman moored at Pier 14S on Naval Station Norfolk; M...\n",
            "    [CVN76] [Bremerton / Kitsap] [Dec. 2] November 12,  USS Ronald Reagan moored at Bravo Pier on Naval Base Kitsap-Bremer...\n",
            "    [CVN77] [Norfolk / Portsmouth] [November 13] November 13, USS George H.W. Bush moored at Pier 11S on Naval Station Norfolk af...\n",
            "    [CVN78] [Caribbean Sea] [Nov. 16] November 13, USS Gerald R. Ford participated in a PHOTOEX with the USS Mahan, US...\n",
            "    [LHD1] [Norfolk / Portsmouth] [May 19] April 14, USS Wasp moored at Berth 6, Pier 11 on Naval Station Norfolk after a 1...\n",
            "    [LHD2] [San Diego] [Oct. 9] October 6, USS Essex departed Naval Base San Diego for sea trials following an e...\n",
            "    [LHD3] [Norfolk / Portsmouth] [Nov. 14] September 26, USS Kearsarge moved from Pier 12 to Berth 3, Pier 14 on Naval Stat...\n",
            "    [LHD4] [San Diego] [Nov. 12] November 4, USS Boxer moored at Berth 6, Pier 13 on Naval Base San Diego for a b...\n",
            "    [LHD5] [Norfolk / Portsmouth] [2025] July ?, 2025 USS Bataan undocked and moored at  Berth 2E on NASSCO shipyard....\n",
            "    [LHD7] [Caribbean Sea] [Nov. 18] November 6, The Iwo Jima moored at Ann E. Abramson Marine Facility in Frederikst...\n",
            "    [LHD8] [San Diego] [Nov. 10] October 18, USS Makin Island participated in Amphibious Capabilities Demonstrati...\n",
            "    [LHA6] [San Diego] [2025] , USS America moored at Berth 5, Pier 13 in its new homeport of Naval Base San D...\n",
            "    [LHA7] [Sasebo] [Oct. 29] October 24, The Tripoli moored at Navy Pier East on White Beach Naval Facility i...\n",
            "    >>> Official data saved to 'big_deck_status.csv'\n",
            "\n",
            "[*] PHASE 2: SOCIAL INTELLIGENCE (OSINT)\n",
            "    > Scanning WarshipCam.net...\n",
            "    > Sniping DuckDuckGo for: '\"USS\" \"2025\"'...\n",
            "    + Found 6 unique social sightings.\n",
            "    >>> Social data saved to 'social_naval_intel.csv'\n",
            "\n",
            "==========================================================================================\n",
            "SUCCESS: Intelligence Cycle Complete.\n",
            "Ready to map with 'master_fleet_mapper.py'\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhbYYCV6H6vN",
        "outputId": "a92e1d26-6883-46f2-9fc6-530a13aab3b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "US NAVY MASTER FLEET MAPPER (v4.0 - Location Conflict Fix)\n",
        "\n",
        "Integrates data from:\n",
        "1. big_deck_status.csv (Official History)\n",
        "2. social_naval_intel.csv (Social Media/WarshipCam)\n",
        "\n",
        "Fixes v4.0:\n",
        "- Manually offsets 'Norfolk' vs 'Newport News' coordinates to prevent\n",
        "  visual stacking of pins in Hampton Roads.\n",
        "- Ensures distinct visibility for all ports.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import folium\n",
        "import os\n",
        "from folium.plugins import MarkerCluster, BeautifyIcon\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OFFICIAL_DATA_FILE = \"big_deck_status.csv\"\n",
        "SOCIAL_DATA_FILE = \"social_naval_intel.csv\"\n",
        "OUTPUT_FILE = \"master_fleet_map.html\"\n",
        "\n",
        "# --- HULL WHITELIST ---\n",
        "TARGET_FLEET = [\n",
        "    \"CVN-68\", \"CVN-69\", \"CVN-70\", \"CVN-71\", \"CVN-72\", \"CVN-73\",\n",
        "    \"CVN-74\", \"CVN-75\", \"CVN-76\", \"CVN-77\", \"CVN-78\",\n",
        "    \"LHD-1\", \"LHD-2\", \"LHD-3\", \"LHD-4\", \"LHD-5\", \"LHD-7\", \"LHD-8\",\n",
        "    \"LHA-6\", \"LHA-7\"\n",
        "]\n",
        "\n",
        "# --- COORDINATE DICTIONARY ---\n",
        "# Maps keywords to [Lat, Lon].\n",
        "LOCATION_COORDS = {\n",
        "    # US Ports / Shipyards\n",
        "    # Offset Norfolk and Newport News slightly to prevent overlap\n",
        "    \"norfolk\": [36.96, -76.30],        # Naval Station (East)\n",
        "    \"portsmouth\": [36.82, -76.30],\n",
        "    \"newport news\": [36.98, -76.46],   # Shipyard (West)\n",
        "    \"virginia beach\": [36.85, -75.97], # Oceana Area\n",
        "\n",
        "    \"san diego\": [32.68, -117.18],\n",
        "    \"north island\": [32.70, -117.20],\n",
        "    \"camp pendleton\": [33.29, -117.40],\n",
        "    \"bremerton\": [47.55, -122.64],\n",
        "    \"kitsap\": [47.70, -122.70],\n",
        "    \"everett\": [47.98, -122.22],\n",
        "    \"mayport\": [30.39, -81.42],\n",
        "    \"pearl harbor\": [21.35, -157.97],\n",
        "    \"pascagoula\": [30.34, -88.56],\n",
        "    \"ingalls\": [30.34, -88.56],\n",
        "    \"groton\": [41.39, -72.08],\n",
        "\n",
        "    # Foreign Ports\n",
        "    \"yokosuka\": [35.29, 139.66],\n",
        "    \"sasebo\": [33.16, 129.71],\n",
        "    \"busan\": [35.10, 129.11],\n",
        "    \"guam\": [13.44, 144.65],\n",
        "    \"singapore\": [1.30, 103.85],\n",
        "    \"changi\": [1.35, 104.00],\n",
        "    \"bahrain\": [26.22, 50.61],\n",
        "    \"dubai\": [25.26, 55.30],\n",
        "    \"jebel ali\": [25.00, 55.06],\n",
        "    \"manila\": [14.59, 120.97],\n",
        "    \"subic\": [14.80, 120.27],\n",
        "    \"okinawa\": [26.30, 127.80],\n",
        "    \"klang\": [3.00, 101.38],\n",
        "    \"trinidad\": [10.66, -61.51],\n",
        "    \"tobago\": [11.25, -60.67],\n",
        "    \"st. croix\": [17.71, -64.88],\n",
        "    \"rostock\": [54.15, 12.10],\n",
        "    \"plymouth\": [50.37, -4.14],\n",
        "    \"oslo\": [59.91, 10.75],\n",
        "    \"split\": [43.50, 16.44],\n",
        "\n",
        "    # Regions / Seas\n",
        "    \"south china sea\": [12.00, 114.00],\n",
        "    \"spratly\": [10.00, 114.00],\n",
        "    \"luzon\": [16.00, 121.00],\n",
        "    \"philippine sea\": [20.00, 130.00],\n",
        "    \"western pacific\": [15.00, 135.00],\n",
        "    \"westpac\": [15.00, 135.00],\n",
        "    \"san bernardino\": [12.53, 124.28],\n",
        "    \"persian gulf\": [27.00, 51.00],\n",
        "    \"arabian gulf\": [27.00, 51.00],\n",
        "    \"red sea\": [20.00, 38.00],\n",
        "    \"gulf of oman\": [24.00, 58.00],\n",
        "    \"gulf of aden\": [12.00, 48.00],\n",
        "    \"mediterranean\": [35.00, 18.00],\n",
        "    \"adriatic\": [42.00, 15.00],\n",
        "    \"caribbean\": [15.00, -75.00],\n",
        "    \"puerto rico\": [18.22, -66.59],\n",
        "    \"north sea\": [56.00, 3.00],\n",
        "    \"norwegian sea\": [66.00, 5.00],\n",
        "    \"gibraltar\": [35.95, -5.60],\n",
        "    \"suez\": [30.60, 32.33],\n",
        "    \"panama canal\": [9.10, -79.67],\n",
        "\n",
        "    # Oceans (Fallback)\n",
        "    \"atlantic\": [33.00, -60.00],\n",
        "    \"pacific\": [25.00, -150.00],\n",
        "    \"indian ocean\": [-5.00, 80.00],\n",
        "}\n",
        "\n",
        "def get_location_key_and_coords(text):\n",
        "    \"\"\"\n",
        "    Scans a raw text string against the coordinate dictionary.\n",
        "    Returns: (location_key, [lat, lon]) or (None, None)\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str): return None, None\n",
        "    text = text.lower()\n",
        "\n",
        "    # Sort keys by length (descending) to ensure \"Newport News\" matches before \"Newport\"\n",
        "    # and avoids partial matches.\n",
        "    sorted_keys = sorted(LOCATION_COORDS.keys(), key=len, reverse=True)\n",
        "\n",
        "    for location in sorted_keys:\n",
        "        if location in text:\n",
        "            return location, LOCATION_COORDS[location]\n",
        "    return None, None\n",
        "\n",
        "def is_target_ship(ship_name, hull):\n",
        "    \"\"\"Checks if a ship from the social feed is in our Big Deck whitelist.\"\"\"\n",
        "    s_hull = str(hull).upper().replace(\" \", \"\")\n",
        "    for target in TARGET_FLEET:\n",
        "        if target.replace(\"-\", \"\") in s_hull.replace(\"-\", \"\"):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def generate_map():\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"MASTER FLEET MAPPER (v4.0 - Fixes Applied)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    locations_db = {}\n",
        "\n",
        "    def add_to_db(loc_key, coords, ship_data):\n",
        "        if loc_key not in locations_db:\n",
        "            locations_db[loc_key] = {\"coords\": coords, \"ships\": []}\n",
        "        locations_db[loc_key][\"ships\"].append(ship_data)\n",
        "\n",
        "    # --- PROCESS OFFICIAL DATA ---\n",
        "    if os.path.exists(OFFICIAL_DATA_FILE):\n",
        "        print(f\"Processing Official Data...\")\n",
        "        df_official = pd.read_csv(OFFICIAL_DATA_FILE)\n",
        "\n",
        "        for _, row in df_official.iterrows():\n",
        "            loc_tag = str(row['Location'])\n",
        "            loc_key, coords = get_location_key_and_coords(loc_tag)\n",
        "\n",
        "            if coords:\n",
        "                add_to_db(loc_key, coords, {\n",
        "                    \"hull\": row['Hull'],\n",
        "                    \"name\": \"\",\n",
        "                    \"date\": row['Date'],\n",
        "                    \"status\": row['Status Sentence'],\n",
        "                    \"source\": \"Official\",\n",
        "                    \"type\": \"CVN\" if \"CVN\" in row['Hull'] else \"LHD\"\n",
        "                })\n",
        "            else:\n",
        "                print(f\"  ! Unmapped Official Ship: {row['Hull']} -> '{loc_tag}'\")\n",
        "\n",
        "    # --- PROCESS SOCIAL DATA ---\n",
        "    if os.path.exists(SOCIAL_DATA_FILE):\n",
        "        print(f\"Processing Social Intel...\")\n",
        "        try:\n",
        "            df_social = pd.read_csv(SOCIAL_DATA_FILE)\n",
        "            for _, row in df_social.iterrows():\n",
        "                ship_name = row.get('Ship', '')\n",
        "                hull = row.get('Hull', '')\n",
        "                raw_text = str(row.get('Activity/Location', '')) + \" \" + str(row.get('Raw Intel', ''))\n",
        "                date_str = row.get('Date', 'Unknown')\n",
        "\n",
        "                if is_target_ship(ship_name, hull):\n",
        "                    loc_key, coords = get_location_key_and_coords(raw_text)\n",
        "                    if coords:\n",
        "                        add_to_db(loc_key, coords, {\n",
        "                            \"hull\": hull,\n",
        "                            \"name\": ship_name,\n",
        "                            \"date\": date_str,\n",
        "                            \"status\": raw_text[:150] + \"...\",\n",
        "                            \"source\": \"OSINT\",\n",
        "                            \"type\": \"CVN\" if \"CVN\" in str(hull).upper() else \"LHD\"\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            print(f\"  ! Error reading social CSV: {e}\")\n",
        "\n",
        "    # --- RENDER MAP ---\n",
        "    m = folium.Map(location=[25, 0], zoom_start=3, tiles=\"CartoDB positron\")\n",
        "\n",
        "    marker_count = 0\n",
        "\n",
        "    for loc_key, data in locations_db.items():\n",
        "        coords = data['coords']\n",
        "        ships = data['ships']\n",
        "\n",
        "        has_cvn = any(s['type'] == \"CVN\" for s in ships)\n",
        "        if has_cvn:\n",
        "            color = \"blue\"\n",
        "            icon = \"plane\"\n",
        "        else:\n",
        "            color = \"red\"\n",
        "            icon = \"anchor\"\n",
        "\n",
        "        hull_list = [s['hull'] for s in ships]\n",
        "        tooltip_text = f\"{loc_key.title()}: {', '.join(hull_list)}\"\n",
        "\n",
        "        popup_html = f\"\"\"\n",
        "        <div style=\"font-family:sans-serif; width:350px; overflow-y:auto; max-height:300px;\">\n",
        "            <h4 style=\"margin-bottom:5px; border-bottom:1px solid #ccc;\">{loc_key.upper()}</h4>\n",
        "            <table style=\"width:100%; font-size:11px; border-collapse:collapse;\">\n",
        "        \"\"\"\n",
        "\n",
        "        for s in ships:\n",
        "            row_color = \"#f9f9f9\" if s['source'] == \"Official\" else \"#eaffea\"\n",
        "            source_badge = \"OFFICIAL\" if s['source'] == \"Official\" else \"SOCIAL\"\n",
        "            badge_color = \"gray\" if s['source'] == \"Official\" else \"green\"\n",
        "\n",
        "            popup_html += f\"\"\"\n",
        "            <tr style=\"background-color:{row_color}; border-bottom:1px solid #eee;\">\n",
        "                <td style=\"padding:5px; vertical-align:top;\">\n",
        "                    <b>{s['hull']}</b><br>\n",
        "                    <span style=\"color:white; background:{badge_color}; padding:1px 3px; border-radius:3px; font-size:9px;\">{source_badge}</span>\n",
        "                </td>\n",
        "                <td style=\"padding:5px;\">\n",
        "                    <b>{s['date']}</b><br>\n",
        "                    {s['status']}\n",
        "                </td>\n",
        "            </tr>\n",
        "            \"\"\"\n",
        "\n",
        "        popup_html += \"</table></div>\"\n",
        "\n",
        "        folium.Marker(\n",
        "            location=coords,\n",
        "            popup=folium.Popup(popup_html, max_width=370),\n",
        "            tooltip=tooltip_text,\n",
        "            icon=folium.Icon(color=color, icon=icon, prefix='fa')\n",
        "        ).add_to(m)\n",
        "        marker_count += 1\n",
        "\n",
        "    # Legend\n",
        "    title_html = '''\n",
        "             <div style=\"position: fixed;\n",
        "                         top: 10px; right: 10px; width: 180px; height: 110px;\n",
        "                         z-index:9999; font-size:13px;\n",
        "                         background-color: white; opacity: 0.9;\n",
        "                         padding: 10px; border-radius: 5px; border: 1px solid grey; box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\">\n",
        "                 <b>US Navy Fleet Overview</b><br>\n",
        "                 <hr style=\"margin:5px 0;\">\n",
        "                 <i class=\"fa fa-plane\" style=\"color:blue; margin-right:5px;\"></i> Carrier Present<br>\n",
        "                 <i class=\"fa fa-anchor\" style=\"color:red; margin-right:5px;\"></i> Amphib Only<br>\n",
        "                 <br>\n",
        "                 <span style=\"font-size:10px;\">*Click pins for ship list</span>\n",
        "             </div>\n",
        "             '''\n",
        "    m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "    m.save(OUTPUT_FILE)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"SUCCESS: Aggregated Map generated at '{os.path.abspath(OUTPUT_FILE)}'\")\n",
        "    print(f\"Mapped {len(locations_db)} unique fleet locations.\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_map()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MKQ-bxzUBdz",
        "outputId": "3d93d82f-3ffb-4181-87e7-8196c07780b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MASTER FLEET MAPPER (v4.0 - Fixes Applied)\n",
            "============================================================\n",
            "Processing Official Data...\n",
            "Processing Social Intel...\n",
            "\n",
            "============================================================\n",
            "SUCCESS: Aggregated Map generated at '/content/master_fleet_map.html'\n",
            "Mapped 8 unique fleet locations.\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}